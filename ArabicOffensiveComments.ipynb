{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-01T15:01:01.152575Z",
     "iopub.status.busy": "2022-06-01T15:01:01.151902Z",
     "iopub.status.idle": "2022-06-01T15:01:01.161222Z",
     "shell.execute_reply": "2022-06-01T15:01:01.160340Z",
     "shell.execute_reply.started": "2022-06-01T15:01:01.152535Z"
    }
   },
   "source": [
    "# Project: The Arabic Offensive and Toxic Comments Detection Project\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- <a href='#intro'>1. Project Overview and Objectives</a> \n",
    "    - <a href='#dataset'>1.1. Data Set Description</a>\n",
    "- <a href=\"#wrangling\">2. Data Wrangling</a>\n",
    "    - <a href='#wranglingArabic'>2.1. Arabic DataSet</a>\n",
    "- <a href=\"#eda\">3. Exploratory Data Analysis</a>\n",
    "     - <a href='#explorArabic'>3.1. Arabic DataSet</a>\n",
    "- <a href='#pre'>4. Preprocessing DataSet</a>\n",
    "    - <a href='#preArabic'>4.1. Arabic DataSet</a>\n",
    "- <a href='#rnn'>5. RNN Model</a>\n",
    "    - <a href='#apply'>5.1. Apply</a>\n",
    "    - <a href='#build'>5.2. Model Building</a>\n",
    "        - <a href='#buildArabic'>5.2.1. Arabic DataSet</a>\n",
    "    - <a href='#perf'>5.3. Model Performance</a>\n",
    "        - <a href='#preformancarabic'>5.3.1. Arabic DataSet</a>\n",
    "    - <a href='#tensor'>5.4. Convert Model Keras To TensorFlow</a>\n",
    "        - <a href='#convertarabic'>5.4.1. Arabic DataSet</a>\n",
    "- <a href='#concl'>. Conclusions</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T15:03:56.376451Z",
     "iopub.status.busy": "2022-06-01T15:03:56.376037Z",
     "iopub.status.idle": "2022-06-01T15:03:56.390908Z",
     "shell.execute_reply": "2022-06-01T15:03:56.389569Z",
     "shell.execute_reply.started": "2022-06-01T15:03:56.376419Z"
    }
   },
   "source": [
    "<a id='intro'></a>\n",
    "## Project Overview and Objectives\n",
    "\n",
    "***\n",
    "Posting offensive or abusive content on social media have been a serious concern in recent years, and the usage of bad words and aggressive words has been increased significantly, The young population is playing a major role in it. Cyberbullying affects more than half of the young population using social media. Insults in social media websites create negative interactions within the network.  This has created a lot of problems because of the huge popularity and usage of social media sites like Facebook and Twitter. Discussing things you care about can be difficult. The threat of abuse and harassment online means that many people stop expressing themselves and give up on seeking different opinions. so we decided to make our application special so The main motivation lies in the fact that our model will automate and accelerate the detection of the posted offensive content and toxic comments to improve online conversation. NLP techniques and sentiment analysis with the help of Machine Learning / Deep learning models, is used for analyzing the social comment and identified the aggressive effect of an individual or a group. An effective model acts as the core component in a final prototype system that can detect cyberbullying on social media.\n",
    "Developing a system to detect online offensive language is very difficult and important to the health and the security of online users. Studies have shown that cyberhate, online harassment and other misuses of technology are on the rise. Particularly during the global Coronavirus pandemic in 2020, 35% reported online harassment related to their identity-based characteristics, which is a 3% increase over 2019.\n",
    "Applying advanced techniques from the Natural Language Processing (NLP) field to support the development of an online hate free community is a critical task for social justice. Transfer learning enhances the performance of the model by allowing the transfer of knowledge from one domain or one dataset to others that have not been seen before, thus, supporting the model to be more generalizable. In our project, we apply the principles of transfer learning cross multiple Arabic offensive language datasets to compare the effects on system performance. The scope of this project covers Arabic text from online user-generated content for build one model . While there are multiple forms of the Arabic language, the majority of the content from user generated platforms are written in dialectic Arabic. The dialectic form of Arabic is the actual spoken Arabic, and it has several categories depending on social and geographical factors. the Arabic dialects are divided into seven categories; Egyptian, Levantine, Gulf, North African, Iraqi, Yemenite,    and Maltese. The diversity among Arabic dialects and Modern dialects  like franco and mix Arabic with English adds difficulties to the process of developing an NLP system that can understand online Arabic content similar to human level of understanding, and there isn’t any way to stemming Arabic words to back the word to its root like English that make it very difficult.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataset'></a>\n",
    "## Data Set Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It conists of two classes:\n",
    "* `Not Offensive` -   encoded as `0`\n",
    "* `Offensive` -   encoded as `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T18:17:18.446713Z",
     "iopub.status.busy": "2022-06-01T18:17:18.446210Z",
     "iopub.status.idle": "2022-06-01T18:17:20.905085Z",
     "shell.execute_reply": "2022-06-01T18:17:20.904321Z",
     "shell.execute_reply.started": "2022-06-01T18:17:18.446677Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:11:56.271834Z",
     "iopub.status.busy": "2022-06-01T17:11:56.271440Z",
     "iopub.status.idle": "2022-06-01T17:12:05.895356Z",
     "shell.execute_reply": "2022-06-01T17:12:05.894108Z",
     "shell.execute_reply.started": "2022-06-01T17:11:56.271803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arabic_reshaper in /opt/conda/lib/python3.7/site-packages (2.1.3)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from arabic_reshaper) (0.18.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from arabic_reshaper) (59.8.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install arabic_reshaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:29:52.308598Z",
     "iopub.status.busy": "2022-06-01T17:29:52.307996Z",
     "iopub.status.idle": "2022-06-01T17:29:52.320832Z",
     "shell.execute_reply": "2022-06-01T17:29:52.320107Z",
     "shell.execute_reply.started": "2022-06-01T17:29:52.308553Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import xgboost as xgb\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding,SpatialDropout1D\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint ,ReduceLROnPlateau ,EarlyStopping\n",
    "from tensorflow import keras\n",
    "from tensorflow import lite\n",
    "import json\n",
    "import pickle\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.metrics import confusion_matrix,precision_score,recall_score,classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import codecs\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wrangling'></a>\n",
    "## Data Wrangling\n",
    "\n",
    "### General Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wranglingArabic'></a>\n",
    "### Arabic DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gathering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:30:15.010350Z",
     "iopub.status.busy": "2022-06-01T17:30:15.009538Z",
     "iopub.status.idle": "2022-06-01T17:30:15.083617Z",
     "shell.execute_reply": "2022-06-01T17:30:15.082848Z",
     "shell.execute_reply.started": "2022-06-01T17:30:15.010315Z"
    }
   },
   "outputs": [],
   "source": [
    "df_comments1 = pd.read_csv(\"../input/arabicoffensive/ar_dataset.csv\", header=None)\n",
    "df_comments2 = pd.read_csv(\"../input/arabicoffensive/bad_comments.csv\", header=None)\n",
    "df_comments3 = pd.read_csv(\"../input/arabicoffensive/dataset_format_with_ref_labels.tsv\",sep='\\t', header=None)\n",
    "df_comments4 = pd.read_csv(\"../input/arabicoffensive/test_instances_batch.tsv\",sep='\\t', header=None)\n",
    "df_comments6 = pd.read_csv(\"../input/additionaldata/toxic arabic tweets classification.tsv\",sep='\\t', header=None)\n",
    "df_comments5 = pd.read_csv(\"../input/arabicoffensive/arabic_offensive_comments.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pre'></a>\n",
    "## Preprocessing DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='preArabic'></a>\n",
    "### Arabic DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:30:20.756260Z",
     "iopub.status.busy": "2022-06-01T17:30:20.755619Z",
     "iopub.status.idle": "2022-06-01T17:30:20.764411Z",
     "shell.execute_reply": "2022-06-01T17:30:20.763227Z",
     "shell.execute_reply.started": "2022-06-01T17:30:20.756223Z"
    }
   },
   "outputs": [],
   "source": [
    "df_comments2[2] = np.where(df_comments2[2]=='0\\n','NOT_OFF','OFF')\n",
    "df_comments2.columns = ['id', 'comment','label']\n",
    "df_comments2=df_comments2.tail(-1)\n",
    "df_comments2.drop('id',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:30:23.607954Z",
     "iopub.status.busy": "2022-06-01T17:30:23.607311Z",
     "iopub.status.idle": "2022-06-01T17:30:23.617234Z",
     "shell.execute_reply": "2022-06-01T17:30:23.616517Z",
     "shell.execute_reply.started": "2022-06-01T17:30:23.607918Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>مبروك و سامحونا لعجزنا التام. عقبال اللي جوه. ...</td>\n",
       "      <td>NOT_OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>كلنا بره ومش هنبطل نزايد على العجايز الي جابون...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>بدل ما انت قاعد بره كده تعالي ازرع الصحرا\\n</td>\n",
       "      <td>NOT_OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>قذر اتفووو ماتيجى مصر وتورينا نفسك كدا ياجبان\\n</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>وهكذا رجال الشو اللي محرومين من عمل برنامج الغ...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment    label\n",
       "1  مبروك و سامحونا لعجزنا التام. عقبال اللي جوه. ...  NOT_OFF\n",
       "2  كلنا بره ومش هنبطل نزايد على العجايز الي جابون...      OFF\n",
       "3        بدل ما انت قاعد بره كده تعالي ازرع الصحرا\\n  NOT_OFF\n",
       "4    قذر اتفووو ماتيجى مصر وتورينا نفسك كدا ياجبان\\n      OFF\n",
       "5  وهكذا رجال الشو اللي محرومين من عمل برنامج الغ...      OFF"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:30:26.648482Z",
     "iopub.status.busy": "2022-06-01T17:30:26.647695Z",
     "iopub.status.idle": "2022-06-01T17:30:26.662233Z",
     "shell.execute_reply": "2022-06-01T17:30:26.661382Z",
     "shell.execute_reply.started": "2022-06-01T17:30:26.648442Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HITId</td>\n",
       "      <td>tweet</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>directness</td>\n",
       "      <td>annotator_sentiment</td>\n",
       "      <td>target</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>صلاة الفجر خير لك من ترديد بول البعير وسبي الن...</td>\n",
       "      <td>hateful_normal</td>\n",
       "      <td>indirect</td>\n",
       "      <td>shock</td>\n",
       "      <td>gender</td>\n",
       "      <td>individual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>صراحة نفسي اشوف ولاد الوسخة اللي قالوا مدرب اج...</td>\n",
       "      <td>offensive</td>\n",
       "      <td>indirect</td>\n",
       "      <td>anger_confusion_sadness_indifference_disgust</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>طيب! هي متبرجة وعبايتها ملونه وطالعة من بيتهم ...</td>\n",
       "      <td>offensive</td>\n",
       "      <td>indirect</td>\n",
       "      <td>indifference</td>\n",
       "      <td>other</td>\n",
       "      <td>individual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@user @user انا اوافقك بخصوص السوريين و العراق...</td>\n",
       "      <td>normal</td>\n",
       "      <td>direct</td>\n",
       "      <td>indifference</td>\n",
       "      <td>origin</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0                                                  1               2  \\\n",
       "0  HITId                                              tweet       sentiment   \n",
       "1      1  صلاة الفجر خير لك من ترديد بول البعير وسبي الن...  hateful_normal   \n",
       "2      2  صراحة نفسي اشوف ولاد الوسخة اللي قالوا مدرب اج...       offensive   \n",
       "3      3  طيب! هي متبرجة وعبايتها ملونه وطالعة من بيتهم ...       offensive   \n",
       "4      4  @user @user انا اوافقك بخصوص السوريين و العراق...          normal   \n",
       "\n",
       "            3                                             4       5  \\\n",
       "0  directness                           annotator_sentiment  target   \n",
       "1    indirect                                         shock  gender   \n",
       "2    indirect  anger_confusion_sadness_indifference_disgust   other   \n",
       "3    indirect                                  indifference   other   \n",
       "4      direct                                  indifference  origin   \n",
       "\n",
       "            6  \n",
       "0       group  \n",
       "1  individual  \n",
       "2       other  \n",
       "3  individual  \n",
       "4       other  "
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:30:30.404673Z",
     "iopub.status.busy": "2022-06-01T17:30:30.403881Z",
     "iopub.status.idle": "2022-06-01T17:30:30.413792Z",
     "shell.execute_reply": "2022-06-01T17:30:30.412565Z",
     "shell.execute_reply.started": "2022-06-01T17:30:30.404630Z"
    }
   },
   "outputs": [],
   "source": [
    "df_comments1.drop([0,3,4,5,6],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:30:36.723993Z",
     "iopub.status.busy": "2022-06-01T17:30:36.723599Z",
     "iopub.status.idle": "2022-06-01T17:30:36.735021Z",
     "shell.execute_reply": "2022-06-01T17:30:36.734049Z",
     "shell.execute_reply.started": "2022-06-01T17:30:36.723959Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>صلاة الفجر خير لك من ترديد بول البعير وسبي الن...</td>\n",
       "      <td>hateful_normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>صراحة نفسي اشوف ولاد الوسخة اللي قالوا مدرب اج...</td>\n",
       "      <td>offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>طيب! هي متبرجة وعبايتها ملونه وطالعة من بيتهم ...</td>\n",
       "      <td>offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@user @user انا اوافقك بخصوص السوريين و العراق...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>هذه السعودية التي شعبها شعب الخيم و بول البعير...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment           label\n",
       "1  صلاة الفجر خير لك من ترديد بول البعير وسبي الن...  hateful_normal\n",
       "2  صراحة نفسي اشوف ولاد الوسخة اللي قالوا مدرب اج...       offensive\n",
       "3  طيب! هي متبرجة وعبايتها ملونه وطالعة من بيتهم ...       offensive\n",
       "4  @user @user انا اوافقك بخصوص السوريين و العراق...          normal\n",
       "5  هذه السعودية التي شعبها شعب الخيم و بول البعير...          normal"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments1=df_comments1.tail(-1)\n",
    "df_comments1.columns = ['comment','label']\n",
    "df_comments1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:30:42.031707Z",
     "iopub.status.busy": "2022-06-01T17:30:42.031222Z",
     "iopub.status.idle": "2022-06-01T17:30:42.043024Z",
     "shell.execute_reply": "2022-06-01T17:30:42.042355Z",
     "shell.execute_reply.started": "2022-06-01T17:30:42.031668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>من اسباب ظاهرة التحرش بالمرأة: @url</td>\n",
       "      <td>NOT_OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>@user @user اولا يا استاذ احترم نفسك عشان اقدر...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3049</th>\n",
       "      <td>@user هم مثل خنازير الذره ...لم يكتفو بالاكل ....</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097</th>\n",
       "      <td>الناس كلها شغالا بي موضوع التحرش والحاجات الحا...</td>\n",
       "      <td>NOT_OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>@user @user @user يا موجز..اللي في القناة الخن...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment    label\n",
       "901                 من اسباب ظاهرة التحرش بالمرأة: @url  NOT_OFF\n",
       "75    @user @user اولا يا استاذ احترم نفسك عشان اقدر...      OFF\n",
       "3049  @user هم مثل خنازير الذره ...لم يكتفو بالاكل ....      OFF\n",
       "3097  الناس كلها شغالا بي موضوع التحرش والحاجات الحا...  NOT_OFF\n",
       "976   @user @user @user يا موجز..اللي في القناة الخن...      OFF"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments1['label'] = np.where(df_comments1['label']=='normal','NOT_OFF','OFF')\n",
    "df_comments1.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:30:46.546300Z",
     "iopub.status.busy": "2022-06-01T17:30:46.545893Z",
     "iopub.status.idle": "2022-06-01T17:30:46.552597Z",
     "shell.execute_reply": "2022-06-01T17:30:46.551453Z",
     "shell.execute_reply.started": "2022-06-01T17:30:46.546248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['OFF', 'NOT_OFF'], dtype=object)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments1['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:31:00.726806Z",
     "iopub.status.busy": "2022-06-01T17:31:00.725915Z",
     "iopub.status.idle": "2022-06-01T17:31:00.732887Z",
     "shell.execute_reply": "2022-06-01T17:31:00.731770Z",
     "shell.execute_reply.started": "2022-06-01T17:31:00.726768Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf_comments6=df_comments6.tail(-1)\\ndf_comments6.columns = ['comment','label']\\ndf_comments6\\n\""
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df_comments6=df_comments6.tail(-1)\n",
    "df_comments6.columns = ['comment','label']\n",
    "df_comments6['label'] = np.where(df_comments6['label']=='normal','NOT_OFF','OFF')\n",
    "df_comments6.sample(5)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:31:29.752726Z",
     "iopub.status.busy": "2022-06-01T17:31:29.752030Z",
     "iopub.status.idle": "2022-06-01T17:31:29.759887Z",
     "shell.execute_reply": "2022-06-01T17:31:29.758496Z",
     "shell.execute_reply.started": "2022-06-01T17:31:29.752685Z"
    }
   },
   "outputs": [],
   "source": [
    "df_comments5['label'] = np.where(df_comments5['label']=='Non-Offensive\\n','NOT_OFF','OFF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:31:31.883369Z",
     "iopub.status.busy": "2022-06-01T17:31:31.882393Z",
     "iopub.status.idle": "2022-06-01T17:31:31.888932Z",
     "shell.execute_reply": "2022-06-01T17:31:31.887936Z",
     "shell.execute_reply.started": "2022-06-01T17:31:31.883324Z"
    }
   },
   "outputs": [],
   "source": [
    "df_comments3.columns = ['id', 'comment','label']\n",
    "df_comments4.columns = ['id', 'comment','label']\n",
    "df_comments5.columns = ['id', 'comment','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:31:40.010766Z",
     "iopub.status.busy": "2022-06-01T17:31:40.009717Z",
     "iopub.status.idle": "2022-06-01T17:31:40.023223Z",
     "shell.execute_reply": "2022-06-01T17:31:40.022497Z",
     "shell.execute_reply.started": "2022-06-01T17:31:40.010718Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>800</td>\n",
       "      <td>فى حاجات مينفعش نلفت نظركوا ليها زى الاصول كده...</td>\n",
       "      <td>NOT_OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>801</td>\n",
       "      <td>RT @USER: وعيون تنادينا تحايل فينا و نقول يا ع...</td>\n",
       "      <td>NOT_OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>802</td>\n",
       "      <td>يا بلادي يا أم البلاد يا بلادي بحبك يا مصر بحب...</td>\n",
       "      <td>NOT_OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>803</td>\n",
       "      <td>RT @USER: يا رب يا قوي يا معين مدّني بالقوة و ...</td>\n",
       "      <td>NOT_OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>804</td>\n",
       "      <td>RT @USER: رحمك الله يا صدام يا بطل ومقدام. URL</td>\n",
       "      <td>NOT_OFF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                            comment    label\n",
       "1  800  فى حاجات مينفعش نلفت نظركوا ليها زى الاصول كده...  NOT_OFF\n",
       "2  801  RT @USER: وعيون تنادينا تحايل فينا و نقول يا ع...  NOT_OFF\n",
       "3  802  يا بلادي يا أم البلاد يا بلادي بحبك يا مصر بحب...  NOT_OFF\n",
       "4  803  RT @USER: يا رب يا قوي يا معين مدّني بالقوة و ...  NOT_OFF\n",
       "5  804     RT @USER: رحمك الله يا صدام يا بطل ومقدام. URL  NOT_OFF"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments3=df_comments3.tail(-1)\n",
    "df_comments4=df_comments4.tail(-1)\n",
    "df_comments3.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:31:43.900560Z",
     "iopub.status.busy": "2022-06-01T17:31:43.899632Z",
     "iopub.status.idle": "2022-06-01T17:31:43.908796Z",
     "shell.execute_reply": "2022-06-01T17:31:43.907810Z",
     "shell.execute_reply.started": "2022-06-01T17:31:43.900523Z"
    }
   },
   "outputs": [],
   "source": [
    "df_comments3.drop('id',axis=1,inplace=True)\n",
    "df_comments4.drop('id',axis=1,inplace=True)\n",
    "df_comments5.drop('id',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:35:01.168340Z",
     "iopub.status.busy": "2022-06-01T17:35:01.167881Z",
     "iopub.status.idle": "2022-06-01T17:35:01.182178Z",
     "shell.execute_reply": "2022-06-01T17:35:01.181418Z",
     "shell.execute_reply.started": "2022-06-01T17:35:01.168309Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.concat([df_comments6,df_comments1,df_comments2,df_comments3,df_comments4,df_comments5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:34:30.076084Z",
     "iopub.status.busy": "2022-06-01T17:34:30.075691Z",
     "iopub.status.idle": "2022-06-01T17:34:30.084785Z",
     "shell.execute_reply": "2022-06-01T17:34:30.084012Z",
     "shell.execute_reply.started": "2022-06-01T17:34:30.076052Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NOT_OFF    3325\n",
       "OFF         675\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments5['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T15:36:47.172313Z",
     "iopub.status.busy": "2022-06-01T15:36:47.171682Z",
     "iopub.status.idle": "2022-06-01T15:36:47.175619Z",
     "shell.execute_reply": "2022-06-01T15:36:47.174869Z",
     "shell.execute_reply.started": "2022-06-01T15:36:47.172265Z"
    }
   },
   "source": [
    "#### View information of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:35:09.670601Z",
     "iopub.status.busy": "2022-06-01T17:35:09.670113Z",
     "iopub.status.idle": "2022-06-01T17:35:09.682079Z",
     "shell.execute_reply": "2022-06-01T17:35:09.681259Z",
     "shell.execute_reply.started": "2022-06-01T17:35:09.670562Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>من الأخير: إن فرضت الضرائب فستفرض على الشعب وإ...</td>\n",
       "      <td>NOT_OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>مؤامرات باريس المخملية</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>@USER @USER @USER @USER يا كبير انت يا مخلص جم...</td>\n",
       "      <td>NOT_OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>فلوس قطر مهمتها السيطره على الحيوانات والبهايم...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>@user @user لكان خليلكن بول البعير</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>معقواله االسبكي  اكبر مخروج   في  مصر مش  معاه...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>@User.IDX اصبحو مرفوضين من جميع الشعوب الحره. \\n</td>\n",
       "      <td>NOT_OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411</th>\n",
       "      <td>قلتا ليو كيف 15سنه دى م قاصر على حسب معتقدتك و...</td>\n",
       "      <td>NOT_OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5613</th>\n",
       "      <td>يلعن روحك يا هنية على هالتصريح الصرّحتو</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>يا قلب يا محروم ، من فرحة الأيام</td>\n",
       "      <td>NOT_OFF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment    label\n",
       "1091  من الأخير: إن فرضت الضرائب فستفرض على الشعب وإ...  NOT_OFF\n",
       "574                             مؤامرات باريس المخملية       OFF\n",
       "332   @USER @USER @USER @USER يا كبير انت يا مخلص جم...  NOT_OFF\n",
       "379   فلوس قطر مهمتها السيطره على الحيوانات والبهايم...      OFF\n",
       "617                  @user @user لكان خليلكن بول البعير      OFF\n",
       "758   معقواله االسبكي  اكبر مخروج   في  مصر مش  معاه...      OFF\n",
       "1538   @User.IDX اصبحو مرفوضين من جميع الشعوب الحره. \\n  NOT_OFF\n",
       "1411  قلتا ليو كيف 15سنه دى م قاصر على حسب معتقدتك و...  NOT_OFF\n",
       "5613            يلعن روحك يا هنية على هالتصريح الصرّحتو      OFF\n",
       "784                    يا قلب يا محروم ، من فرحة الأيام  NOT_OFF"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:35:14.715897Z",
     "iopub.status.busy": "2022-06-01T17:35:14.714949Z",
     "iopub.status.idle": "2022-06-01T17:35:14.723473Z",
     "shell.execute_reply": "2022-06-01T17:35:14.722618Z",
     "shell.execute_reply.started": "2022-06-01T17:35:14.715857Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['OFF', 'NOT_OFF'], dtype=object)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:35:17.557533Z",
     "iopub.status.busy": "2022-06-01T17:35:17.557053Z",
     "iopub.status.idle": "2022-06-01T17:35:17.566757Z",
     "shell.execute_reply": "2022-06-01T17:35:17.565797Z",
     "shell.execute_reply.started": "2022-06-01T17:35:17.557493Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OFF        12610\n",
       "NOT_OFF     9684\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eda'></a>\n",
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='explorArabic'></a>\n",
    "### Arabic DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T18:16:46.241575Z",
     "iopub.status.busy": "2022-06-01T18:16:46.241083Z",
     "iopub.status.idle": "2022-06-01T18:16:48.767685Z",
     "shell.execute_reply": "2022-06-01T18:16:48.766616Z",
     "shell.execute_reply.started": "2022-06-01T18:16:46.241539Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from wordcloud import WordCloud          # pip install wordcloud\n",
    "import matplotlib.pyplot as plt          \n",
    "# -- Arabic text dependencies\n",
    "from arabic_reshaper import reshape      # pip install arabic-reshaper\n",
    "from bidi.algorithm import get_display   # pip install python-bidi\n",
    "\n",
    "rtl = lambda w: get_display(reshape(f'{w}'))\n",
    "\n",
    "COUNTS = Counter((\" \").join(data['cleaned_text']).split())\n",
    "counts = {rtl(k):v for k, v in COUNTS.most_common(500)}\n",
    "\n",
    "font_file = '../input/fontttttt/NotoNaskhArabic-Regular.ttf' # download from: https://www.google.com/get/noto\n",
    "wordcloud = WordCloud(font_path=font_file).generate_from_frequencies(counts)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:35:30.279463Z",
     "iopub.status.busy": "2022-06-01T17:35:30.278170Z",
     "iopub.status.idle": "2022-06-01T17:35:30.284113Z",
     "shell.execute_reply": "2022-06-01T17:35:30.283128Z",
     "shell.execute_reply.started": "2022-06-01T17:35:30.279390Z"
    }
   },
   "outputs": [],
   "source": [
    "#pip install ar_wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pre'></a>\n",
    "## Preprocessing Text For Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prepromodelArabic'></a>\n",
    "### Arabic DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:35:33.444462Z",
     "iopub.status.busy": "2022-06-01T17:35:33.443177Z",
     "iopub.status.idle": "2022-06-01T17:35:33.449849Z",
     "shell.execute_reply": "2022-06-01T17:35:33.449031Z",
     "shell.execute_reply.started": "2022-06-01T17:35:33.444405Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('arabic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:35:35.663604Z",
     "iopub.status.busy": "2022-06-01T17:35:35.662847Z",
     "iopub.status.idle": "2022-06-01T17:35:35.782237Z",
     "shell.execute_reply": "2022-06-01T17:35:35.780598Z",
     "shell.execute_reply.started": "2022-06-01T17:35:35.663553Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_diacritics(text):\n",
    "    text = text.replace('@User.IDX',' ')\n",
    "    text = text.replace('@user',' ')\n",
    "    \n",
    "    arabic_diacritics = re.compile(\"\"\" ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    text = re.sub(arabic_diacritics, '', str(text))\n",
    "    return text\n",
    "\n",
    "def remove_emoji(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "def remove_urls (text):\n",
    "    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def remove_emails(text):\n",
    "    text = re.sub(r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\", \"\",  text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def remove_english(text):\n",
    "    text = re.sub(r'[a-zA-Z]',\"\",text,flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "#Text Normalization\n",
    "def normalize_arabic(text):\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "from nltk.corpus import words\n",
    "english = words.words()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    text = remove_emoji(text)\n",
    "    text = remove_diacritics(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_emails(text)\n",
    "    text = normalize_arabic(text)\n",
    "    text = remove_english(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    text = ' '.join([word for word in tokens if word not in stop_words])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:35:36.975231Z",
     "iopub.status.busy": "2022-06-01T17:35:36.974573Z",
     "iopub.status.idle": "2022-06-01T17:35:41.000995Z",
     "shell.execute_reply": "2022-06-01T17:35:40.999893Z",
     "shell.execute_reply.started": "2022-06-01T17:35:36.975181Z"
    }
   },
   "outputs": [],
   "source": [
    "data['cleaned_text'] = data['comment'].apply(clean_text)\n",
    "data['label'] = np.where(data['label']=='NOT_OFF',0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:35:41.004768Z",
     "iopub.status.busy": "2022-06-01T17:35:41.003927Z",
     "iopub.status.idle": "2022-06-01T17:35:41.019236Z",
     "shell.execute_reply": "2022-06-01T17:35:41.018237Z",
     "shell.execute_reply.started": "2022-06-01T17:35:41.004717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3139</th>\n",
       "      <td>@user يلا يا بتوع بول البعير يا شواذ يا ولاد ا...</td>\n",
       "      <td>1</td>\n",
       "      <td>يلا بتوع بول البعير شواذ ولاد الوسخه يللي نسون...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>@User.IDX عندي احسن حكم عادل بإذن الله، يتم جم...</td>\n",
       "      <td>0</td>\n",
       "      <td>عندي احسن حكم عادل باذن الله، يتم جمعهم ابمتهم...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5963</th>\n",
       "      <td>اصبري بس و هنيكك يبنت الاحبة</td>\n",
       "      <td>1</td>\n",
       "      <td>اصبري هنيكك يبنت الاحبه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>فيني عاده من وانا صغير اذا حبيت شيء بقوه .. اح...</td>\n",
       "      <td>0</td>\n",
       "      <td>فيني عاده وانا صغير اذا حبيت شيء بقوه احاول اع...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5983</th>\n",
       "      <td>هنيك الشرموطه امك</td>\n",
       "      <td>1</td>\n",
       "      <td>هنيك الشرموطه امك</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>تحمست ..كمل حكاية هتلر ☺ \\n</td>\n",
       "      <td>0</td>\n",
       "      <td>تحمست كمل حكايه هتلر ☺</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>@User.IDX سيبولنا ثرواتنا وتو نشوفو شكون الي ي...</td>\n",
       "      <td>1</td>\n",
       "      <td>سيبولنا ثرواتنا وتو نشوفو شكون الي يجي للثاني ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>@User.IDX انت شوية معرصين ياجزيره مصر اكبر منك...</td>\n",
       "      <td>1</td>\n",
       "      <td>انت شويه معرصين ياجزيره مصر اكبر منكم ياكلاب ت...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2190</th>\n",
       "      <td>لازم تاخدها جماهير البيرو علي الحريم المفخدة ا...</td>\n",
       "      <td>1</td>\n",
       "      <td>لازم تاخدها جماهير البيرو علي الحريم المفخده ا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>RT @USER: يا #همس يا #همس الهوى&lt;LF&gt;في مسمع الغ...</td>\n",
       "      <td>0</td>\n",
       "      <td>همس همس الهويفي مسمع الغ❤الي ابيكتقول مالك لوي...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment  label  \\\n",
       "3139  @user يلا يا بتوع بول البعير يا شواذ يا ولاد ا...      1   \n",
       "2498  @User.IDX عندي احسن حكم عادل بإذن الله، يتم جم...      0   \n",
       "5963                       اصبري بس و هنيكك يبنت الاحبة      1   \n",
       "541   فيني عاده من وانا صغير اذا حبيت شيء بقوه .. اح...      0   \n",
       "5983                                  هنيك الشرموطه امك      1   \n",
       "2995                        تحمست ..كمل حكاية هتلر ☺ \\n      0   \n",
       "1482  @User.IDX سيبولنا ثرواتنا وتو نشوفو شكون الي ي...      1   \n",
       "1487  @User.IDX انت شوية معرصين ياجزيره مصر اكبر منك...      1   \n",
       "2190  لازم تاخدها جماهير البيرو علي الحريم المفخدة ا...      1   \n",
       "487   RT @USER: يا #همس يا #همس الهوى<LF>في مسمع الغ...      0   \n",
       "\n",
       "                                           cleaned_text  \n",
       "3139  يلا بتوع بول البعير شواذ ولاد الوسخه يللي نسون...  \n",
       "2498  عندي احسن حكم عادل باذن الله، يتم جمعهم ابمتهم...  \n",
       "5963                            اصبري هنيكك يبنت الاحبه  \n",
       "541   فيني عاده وانا صغير اذا حبيت شيء بقوه احاول اع...  \n",
       "5983                                  هنيك الشرموطه امك  \n",
       "2995                             تحمست كمل حكايه هتلر ☺  \n",
       "1482  سيبولنا ثرواتنا وتو نشوفو شكون الي يجي للثاني ...  \n",
       "1487  انت شويه معرصين ياجزيره مصر اكبر منكم ياكلاب ت...  \n",
       "2190  لازم تاخدها جماهير البيرو علي الحريم المفخده ا...  \n",
       "487   همس همس الهويفي مسمع الغ❤الي ابيكتقول مالك لوي...  "
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rnn'></a>\n",
    "## RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='RNNArabic'></a>\n",
    "### Arabic DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:35:44.770296Z",
     "iopub.status.busy": "2022-06-01T17:35:44.769863Z",
     "iopub.status.idle": "2022-06-01T17:35:44.793597Z",
     "shell.execute_reply": "2022-06-01T17:35:44.792882Z",
     "shell.execute_reply.started": "2022-06-01T17:35:44.770241Z"
    }
   },
   "outputs": [],
   "source": [
    "X_trainAR, X_testAR, y_trainAR, y_testAR = train_test_split(data['cleaned_text'], data['label'], test_size=0.2, stratify=data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T15:59:58.248800Z",
     "iopub.status.busy": "2022-06-01T15:59:58.248354Z",
     "iopub.status.idle": "2022-06-01T15:59:58.255803Z",
     "shell.execute_reply": "2022-06-01T15:59:58.254195Z",
     "shell.execute_reply.started": "2022-06-01T15:59:58.248765Z"
    }
   },
   "source": [
    "<a id='buildArabic'></a>\n",
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:35:51.644928Z",
     "iopub.status.busy": "2022-06-01T17:35:51.644359Z",
     "iopub.status.idle": "2022-06-01T17:35:52.217903Z",
     "shell.execute_reply": "2022-06-01T17:35:52.216702Z",
     "shell.execute_reply.started": "2022-06-01T17:35:51.644883Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=False)\n",
    "X_train_tfidf = tfidf.fit_transform(X_trainAR)\n",
    "X_test_tfidf = tfidf.transform(X_testAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:35:53.995374Z",
     "iopub.status.busy": "2022-06-01T17:35:53.994924Z",
     "iopub.status.idle": "2022-06-01T17:36:28.881316Z",
     "shell.execute_reply": "2022-06-01T17:36:28.880262Z",
     "shell.execute_reply.started": "2022-06-01T17:35:53.995329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier()\n",
      "0.8874187037452343\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf_tfidf = rf.fit(X_train_tfidf, y_trainAR)\n",
    "print(rf_tfidf)\n",
    "# calculating accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred=rf.predict(X_test_tfidf)\n",
    "print(accuracy_score(y_testAR, y_pred))\n",
    "# save the model to disk\n",
    "filename = 'RandomForest.sav'\n",
    "pickle.dump(rf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:36:45.360786Z",
     "iopub.status.busy": "2022-06-01T17:36:45.360328Z",
     "iopub.status.idle": "2022-06-01T17:36:51.747820Z",
     "shell.execute_reply": "2022-06-01T17:36:51.746736Z",
     "shell.execute_reply.started": "2022-06-01T17:36:45.360741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier()\n",
      "0.859161246916349\n"
     ]
    }
   ],
   "source": [
    "dr = tree.DecisionTreeClassifier()\n",
    "dr_tfidf = dr.fit(X_train_tfidf, y_trainAR)\n",
    "print(dr_tfidf)\n",
    "# calculating accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred=dr.predict(X_test_tfidf)\n",
    "print(accuracy_score(y_testAR, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:36:59.136172Z",
     "iopub.status.busy": "2022-06-01T17:36:59.135527Z",
     "iopub.status.idle": "2022-06-01T17:36:59.243049Z",
     "shell.execute_reply": "2022-06-01T17:36:59.242037Z",
     "shell.execute_reply.started": "2022-06-01T17:36:59.136120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1, solver='liblinear')\n",
      "0.8679076026014801\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver='liblinear', C=1)\n",
    "logreg_tfidf = logreg.fit(X_train_tfidf, y_trainAR)\n",
    "print(logreg_tfidf)\n",
    "# calculating accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred=logreg.predict(X_test_tfidf)\n",
    "print(accuracy_score(y_testAR, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T18:01:40.959211Z",
     "iopub.status.busy": "2022-06-01T18:01:40.958154Z",
     "iopub.status.idle": "2022-06-01T18:03:47.566906Z",
     "shell.execute_reply": "2022-06-01T18:03:47.565759Z",
     "shell.execute_reply.started": "2022-06-01T18:01:40.959169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=5)\n",
      "0.8975106526126935\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(probability=False,C=5)\n",
    "svm_tfidf = svm.fit(X_train_tfidf, y_trainAR)\n",
    "print(svm_tfidf)\n",
    "# calculating accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred=svm.predict(X_test_tfidf)\n",
    "print(accuracy_score(y_testAR, y_pred))\n",
    "# save the model to disk\n",
    "filename = 'SVM.sav'\n",
    "pickle.dump(svm, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial NB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:39:56.929743Z",
     "iopub.status.busy": "2022-06-01T17:39:56.929338Z",
     "iopub.status.idle": "2022-06-01T17:39:56.949915Z",
     "shell.execute_reply": "2022-06-01T17:39:56.948869Z",
     "shell.execute_reply.started": "2022-06-01T17:39:56.929711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n",
      "0.8753083651042834\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb_tfidf = mnb.fit(X_train_tfidf, y_trainAR)\n",
    "print(mnb_tfidf)\n",
    "# calculating accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred=mnb.predict(X_test_tfidf)\n",
    "print(accuracy_score(y_testAR, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T16:16:54.532560Z",
     "iopub.status.busy": "2022-06-01T16:16:54.531654Z",
     "iopub.status.idle": "2022-06-01T16:16:54.535859Z",
     "shell.execute_reply": "2022-06-01T16:16:54.534949Z",
     "shell.execute_reply.started": "2022-06-01T16:16:54.532525Z"
    }
   },
   "source": [
    "### KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:40:03.337662Z",
     "iopub.status.busy": "2022-06-01T17:40:03.337217Z",
     "iopub.status.idle": "2022-06-01T17:40:07.340827Z",
     "shell.execute_reply": "2022-06-01T17:40:07.339684Z",
     "shell.execute_reply.started": "2022-06-01T17:40:03.337624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_jobs=4, n_neighbors=7, weights='distance')\n",
      "0.8631980264633325\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7, weights='distance', n_jobs=4)\n",
    "knn_tfidf = knn.fit(X_train_tfidf, y_trainAR)\n",
    "print(knn_tfidf)\n",
    "# calculating accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred=knn.predict(X_test_tfidf)\n",
    "print(accuracy_score(y_testAR, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:40:12.541117Z",
     "iopub.status.busy": "2022-06-01T17:40:12.540475Z",
     "iopub.status.idle": "2022-06-01T17:43:00.599853Z",
     "shell.execute_reply": "2022-06-01T17:43:00.598601Z",
     "shell.execute_reply.started": "2022-06-01T17:40:12.541068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_jobs=4, n_neighbors=7, weights='distance')\n",
      "0.870374523435748\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(32, 32, 32), activation='relu', solver='adam', max_iter=50)\n",
    "mlp_tfidf = mlp.fit(X_train_tfidf, y_trainAR)\n",
    "print(knn_tfidf)\n",
    "# calculating accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred=mlp.predict(X_test_tfidf)\n",
    "print(accuracy_score(y_testAR, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:44:10.454626Z",
     "iopub.status.busy": "2022-06-01T17:44:10.453972Z",
     "iopub.status.idle": "2022-06-01T17:44:11.441758Z",
     "shell.execute_reply": "2022-06-01T17:44:11.440618Z",
     "shell.execute_reply.started": "2022-06-01T17:44:10.454573Z"
    }
   },
   "outputs": [],
   "source": [
    "max_words = 50000\n",
    "max_len = 300\n",
    "tokenizerAR = Tokenizer(num_words=max_words)\n",
    "tokenizerAR.fit_on_texts(X_trainAR)\n",
    "sequences = tokenizerAR.texts_to_sequences(X_trainAR)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:44:13.484736Z",
     "iopub.status.busy": "2022-06-01T17:44:13.484251Z",
     "iopub.status.idle": "2022-06-01T17:44:13.688046Z",
     "shell.execute_reply": "2022-06-01T17:44:13.687042Z",
     "shell.execute_reply.started": "2022-06-01T17:44:13.484697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 300, 100)          5000000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 300, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 5,080,501\n",
      "Trainable params: 5,080,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelAR = Sequential()\n",
    "modelAR.add(Embedding(max_words, 100, input_length=max_len))\n",
    "modelAR.add(SpatialDropout1D(0.2))\n",
    "modelAR.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "modelAR.add(Dense(1, activation='sigmoid'))\n",
    "modelAR.summary()\n",
    "modelAR.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:44:14.934639Z",
     "iopub.status.busy": "2022-06-01T17:44:14.934209Z",
     "iopub.status.idle": "2022-06-01T17:44:14.941016Z",
     "shell.execute_reply": "2022-06-01T17:44:14.939900Z",
     "shell.execute_reply.started": "2022-06-01T17:44:14.934604Z"
    }
   },
   "outputs": [],
   "source": [
    "early = EarlyStopping(monitor='val_loss', mode='min', patience=4)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience = 2, verbose=1,factor=0.3, min_lr=0.000001)\n",
    "\n",
    "checkpoint= ModelCheckpoint(\n",
    "    filepath='./',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "callbacks_list = [ early, learning_rate_reduction,checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:44:18.908798Z",
     "iopub.status.busy": "2022-06-01T17:44:18.908373Z",
     "iopub.status.idle": "2022-06-01T17:58:10.718423Z",
     "shell.execute_reply": "2022-06-01T17:58:10.717589Z",
     "shell.execute_reply.started": "2022-06-01T17:44:18.908764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "14/14 [==============================] - 88s 6s/step - loss: 0.6590 - accuracy: 0.6058 - val_loss: 0.6126 - val_accuracy: 0.6919\n",
      "Epoch 2/10\n",
      "14/14 [==============================] - 81s 6s/step - loss: 0.5401 - accuracy: 0.7623 - val_loss: 0.5021 - val_accuracy: 0.8052\n",
      "Epoch 3/10\n",
      "14/14 [==============================] - 82s 6s/step - loss: 0.3449 - accuracy: 0.8884 - val_loss: 0.3662 - val_accuracy: 0.8416\n",
      "Epoch 4/10\n",
      "14/14 [==============================] - 82s 6s/step - loss: 0.2258 - accuracy: 0.9284 - val_loss: 0.3372 - val_accuracy: 0.8542\n",
      "Epoch 5/10\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.1791 - accuracy: 0.9460 - val_loss: 0.3369 - val_accuracy: 0.8528\n",
      "Epoch 6/10\n",
      "14/14 [==============================] - 82s 6s/step - loss: 0.1185 - accuracy: 0.9622 - val_loss: 0.3325 - val_accuracy: 0.8657\n",
      "Epoch 7/10\n",
      "14/14 [==============================] - 84s 6s/step - loss: 0.0859 - accuracy: 0.9746 - val_loss: 0.3528 - val_accuracy: 0.8663\n",
      "Epoch 8/10\n",
      "14/14 [==============================] - 84s 6s/step - loss: 0.0665 - accuracy: 0.9803 - val_loss: 0.3879 - val_accuracy: 0.8607\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "Epoch 9/10\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.0447 - accuracy: 0.9873 - val_loss: 0.3959 - val_accuracy: 0.8646\n",
      "Epoch 10/10\n",
      "14/14 [==============================] - 82s 6s/step - loss: 0.0403 - accuracy: 0.9870 - val_loss: 0.3949 - val_accuracy: 0.8660\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "epochs = 10\n",
    "validation_split = 0.2\n",
    "\n",
    "history = modelAR.fit(sequences_matrix,\n",
    "                  y_trainAR,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  validation_split=validation_split,\n",
    "                  callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:58:10.720851Z",
     "iopub.status.busy": "2022-06-01T17:58:10.720220Z",
     "iopub.status.idle": "2022-06-01T17:58:10.841293Z",
     "shell.execute_reply": "2022-06-01T17:58:10.840128Z",
     "shell.execute_reply.started": "2022-06-01T17:58:10.720817Z"
    }
   },
   "outputs": [],
   "source": [
    "modelAR.save(\"Hate&abusive_modelAR.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='perf'></a>\n",
    "## Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='preformancarabic'></a>\n",
    "### Arabic DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T18:06:46.673454Z",
     "iopub.status.busy": "2022-06-01T18:06:46.672851Z",
     "iopub.status.idle": "2022-06-01T18:06:46.695044Z",
     "shell.execute_reply": "2022-06-01T18:06:46.694085Z",
     "shell.execute_reply.started": "2022-06-01T18:06:46.673415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "OFF\n"
     ]
    }
   ],
   "source": [
    "cleaned = clean_text('انت زباله')\n",
    "trans = tfidf.transform([cleaned])\n",
    "x = rf.predict(trans)[0]\n",
    "print(x)\n",
    "if x == 0:\n",
    "    print(\"NOT_OFF\")\n",
    "else:\n",
    "    print(\"OFF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T18:03:47.568891Z",
     "iopub.status.busy": "2022-06-01T18:03:47.568558Z",
     "iopub.status.idle": "2022-06-01T18:03:54.644989Z",
     "shell.execute_reply": "2022-06-01T18:03:54.643915Z",
     "shell.execute_reply.started": "2022-06-01T18:03:47.568862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.905 / Recall: 0.915 / Accuracy: 0.898\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm_tfidf.predict(X_test_tfidf)\n",
    "precision = precision_score(y_testAR, y_pred)\n",
    "recall = recall_score(y_testAR, y_pred)\n",
    "accuracy = accuracy_score(y_testAR, y_pred)\n",
    "print('Precision: {} / Recall: {} / Accuracy: {}'.format(round(precision, 3), round(recall, 3), round(accuracy, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T18:06:29.365858Z",
     "iopub.status.busy": "2022-06-01T18:06:29.365461Z",
     "iopub.status.idle": "2022-06-01T18:06:29.537057Z",
     "shell.execute_reply": "2022-06-01T18:06:29.534967Z",
     "shell.execute_reply.started": "2022-06-01T18:06:29.365823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['انت زباله']\n",
      "[[4, 407]]\n",
      "pred [[0.98784804]]\n",
      "hate and abusive\n"
     ]
    }
   ],
   "source": [
    "test = 'انت زباله '\n",
    "test=[clean_text(test)]\n",
    "print(test)\n",
    "seq = tokenizerAR.texts_to_sequences(test)\n",
    "padded = sequence.pad_sequences(seq, maxlen=300)\n",
    "print(seq)\n",
    "pred = modelAR.predict(padded)\n",
    "print(\"pred\", pred)\n",
    "if pred<0.5:\n",
    "    print(\"no hate\")\n",
    "else:\n",
    "    print(\"hate and abusive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T17:58:45.829708Z",
     "iopub.status.busy": "2022-06-01T17:58:45.829055Z",
     "iopub.status.idle": "2022-06-01T17:58:45.896977Z",
     "shell.execute_reply": "2022-06-01T17:58:45.895973Z",
     "shell.execute_reply.started": "2022-06-01T17:58:45.829664Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('tokenizerHateAR.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizerAR, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tensor'></a>\n",
    "## Convert Model Keras To TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='convertarabic'></a>\n",
    "### Arabic DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T18:00:37.620427Z",
     "iopub.status.busy": "2022-06-01T18:00:37.620050Z",
     "iopub.status.idle": "2022-06-01T18:00:42.657393Z",
     "shell.execute_reply": "2022-06-01T18:00:42.656595Z",
     "shell.execute_reply.started": "2022-06-01T18:00:37.620393Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 18:00:41.863954: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2022-06-01 18:00:41.864225: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2022-06-01 18:00:41.879898: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: Graph size after: 127 nodes (0), 167 edges (0), time = 2.865ms.\n",
      "  function_optimizer: Graph size after: 127 nodes (0), 167 edges (0), time = 2.9ms.\n",
      "Optimization results for grappler item: sequential_2_lstm_2_while_body_34001\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.012ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "Optimization results for grappler item: sequential_2_lstm_2_while_cond_34000\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.006ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.002ms.\n",
      "\n",
      "2022-06-01 18:00:42.332302: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\n",
      "2022-06-01 18:00:42.332372: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20456620"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow import lite\n",
    "saved_model_dir = './Hate&abusive_modelAR.h5'\n",
    "model = keras.models.load_model(saved_model_dir)\n",
    "converter = lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "open(\"Hate&Abusive_ModelAR.tflite\",\"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T18:00:42.659832Z",
     "iopub.status.busy": "2022-06-01T18:00:42.658959Z",
     "iopub.status.idle": "2022-06-01T18:00:42.672644Z",
     "shell.execute_reply": "2022-06-01T18:00:42.671474Z",
     "shell.execute_reply.started": "2022-06-01T18:00:42.659782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 input(s):\n",
      "[  1 300] <class 'numpy.float32'>\n",
      "\n",
      "1 output(s):\n",
      "[1 1] <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=\"./Hate&Abusive_ModelAR.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Print input shape and type\n",
    "inputs = interpreter.get_input_details()\n",
    "print('{} input(s):'.format(len(inputs)))\n",
    "for i in range(0, len(inputs)):\n",
    "    print('{} {}'.format(inputs[i]['shape'], inputs[i]['dtype']))\n",
    "\n",
    "# Print output shape and type\n",
    "outputs = interpreter.get_output_details()\n",
    "print('\\n{} output(s):'.format(len(outputs)))\n",
    "for i in range(0, len(outputs)):\n",
    "    print('{} {}'.format(outputs[i]['shape'], outputs[i]['dtype']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='concl'></a>\n",
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The main focus of NLP project is for detection of offensive and abusive language in online communication conducted so far is mainly for English. There is no much attention being paid to the same problem in Arabic. Therefore, this project is conducted to enrich the current results in finding a solution that would contribute to the reduction of this phenomenon.\n",
    "\n",
    "> In this project, we conduct machine learning experiments with a dataset of YouTube, twitter, and other platforms comments in Arabic. To the best of our knowledge, our dataset is the largest dataset of Arabic text specifically designed for training predictive models for detection of offensive language in online communication.\n",
    "\n",
    "> We used five different datasets to make model learn different Arabic dialectic but Increasing the dataset size for training model not always improve the performance of the system. Thus, finding some other methods to improve the performance are required. For example, Using LSTM instead of a simple RNN, or using SVM Classifier instead of Random Forest and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
